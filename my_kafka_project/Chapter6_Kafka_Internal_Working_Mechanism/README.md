## 카프카 내부 메커니즘

### 클러스터 멤버십

---

- 카프카는 현재 클러스터의 멤버인 브로커들의 목록을 유지하기 위해 **아파치 주키퍼**를 사용
- **각 브로커는** <span style='color:#f7b731'>브로커 설정 파일에 정의</span>되었거나 아니면 <span style='color:#f7b731'>자동으로 생성된 고유한 식별자</span>를 지님
- 브로커 프로세스는 시작될 때마다 <span style='color:#f7b731'>주키퍼에 Ephemeral(임시) 노드의 형태로 ID를 등록</span>

---

- 컨트롤러를 포함한 카프카 브로커들과 몇몇 생태계 툴들은 브로커가 등록되는 주키퍼의 /brokers/ids 경로를 구독함으로써 브로커가 추가되거나 제거될 때마다 알림을 받음
- 만약 동일한 ID를 가진 다른 브로커를 시작한다면 에러가 발생
	- 새 브로커는 자신의 ID 를 등록하려 하겠지만, 이미 동일한 브로커 ID 를 갖는 **Znode**가 있기 때문에 실패

---

> **znode**는 클러스터를 구성하고 있는 각각의 서버(컴퓨터)를 뜻 함 - 
> 1. **Persistent znode**: 명시적으로 제거되기 전에는 계속해서 zookeeper의 데이터 트리에 남아있는 데이터 노드
> 2. **Ephemeral znode**: 세션에 종속되어, znode를 만든 클라이언트와의 세션이 종료되면 삭제되는 일시적인 데이터 노드

---

- 브로커와 주키퍼 간의 연결이 끊어질 경우, <span style='color:#f7b731'>Ephemeral 노드는 자동으로 주키퍼에서 삭제</span>
- 브로커가 정지하면 브로커를 나타내는 Znode가 삭제되지만, 브로커의 ID는 다른 자료구조에 의해서 남아 있게됨
	- 각 토픽의 레플리카 목록에는 해당 레플리카를 저장하는 브로커의 ID가 포함됨
	- 그렇기 때문에 만약 특정한 브로커가 완전히 유실되어 동일한 ID를 가진 새로운 브로커를 투입할 경우, 곧바로 클러스터에서 유실된 브로커의 자리를 대신해서 이전 브로커의 토픽과 파티션들을 할당 받음

---

### 컨트롤러

---

- 클러스터에서 **가장 먼저 시작되는 브로커**는 주키퍼의 <span style='color:#f7b731'>/controller 에 Ephemeral노드를 생성함으로써 컨트롤러</span>가 됨
- 컨트롤러는 일반적인 카프카 브로커의 기능을 하면서, <span style='color:#f7b731'>파티션 리더를 선출하는 역할</span>을 추가적으로 맡음

---

- 다른 브로커 역시 시작할 때 해당 위치에 노드를 생성하려고 시도하지만, <span style='color:#f7b731'>'노드가 이미 존재함' 예외를 받게 되기 때문에 컨트롤러 노드가 이미 존재</span>한다는 것을 알아차리게 됨
- 브로커들은 주키퍼의 <span style='color:#f7b731'>컨트롤러 노드에 뭔가 변동이 생겼을 때 알림을 받기 위해</span>서 이 **노드에 와치를 설정**함
- 이 때문에 클러스터 안에 **단 한개의 컨트롤러만 존재**할 수 있도록 보장할 수 있음

---

- 컨트롤러 브로커가 멈추거나 주키퍼와의  연결이 끊어질 경우, 이 **Ephermal 노드는 삭제**됨
- Ephermal 노드가 삭제될 경우, 클러스터 안의 <span style='color:#f7b731'>다른 브로커들은 주키퍼에 설정된 와치를 통해 컨트롤러가 없어졌다는 것을 알아차리게 됨</span>
- 주키퍼에 <span style='color:#f7b731'>가장 먼저 새로운 노드를 생성하는데 성공한 브로커가 다음 컨트롤러</span>가 되며, 다른 브로커들은 이전처럼 '노드가 이미 존재함' 예외를 받고 **새 컨트롤러 노드에 대한 와치를 다시 생성**

---

- 브로커는 새로운 컨트롤러가 선출될 때마다 **주키퍼의 조건적 증가 연산**에 의해 증가된 에포크 값 을 전달 받음
- 브로커는 현재 컨트롤러의 에포크 값을 알고 있기 때문에, 만약 <span style='color:#f7b731'>더 낮은 에포크 값을 가진 컨트롤러로부터 메시지를 받을 경우 무시</span>함

---

> 이것은 컨트롤러 브로커가 오랫동안 가비지 수집 때문에 멈춘 사이 주키퍼 사이의 연결이 끊어질 수 있기 때문에 중요하다. 이전 컨트롤러가 작업을 재개할 경우, 새로운 컨트롤러가 선출 되었다는 것을 알지 못한 채 브로커에 메시지를 보낼 수 있다. **이러한 컨트롤러를 좀비**라고 부르는데, 컨트롤러가 전송하는 메시지에 컨트롤러 에포크를 포함하면 브로커는 예전 컨트롤러가 보내온 메시지를 무시할 수 있다.

---

- 브로커가 컨트롤러가 되면, 클러스터 메타데이터 관리와 리더 선출을 시작하기 전에 먼저 주키퍼로부터 최신 레플리카 상태 맵을 읽어옴
- 이 적재 작업은 <span style='color:#f7b731'>비동기 API를 사용해서 수행</span>되며, 지연을 줄이기 위해 읽기 요청을 여러 단계로 나눠서 주키퍼로 보냄

---

- 브로커가 클러스터를 나갔다는 사실을 컨트롤러가 알아차리면, <span style='color:#f7b731'>컨트롤러는 해당 브로커가 리더를 맡고 있었던 모든 파티션에 대해 새로운 브로커를 할당</span>해줌 
- 컨트롤러는 새로운 리더가 필요한<span style='color:#f7b731'> 모든 파티션을 순회해 가면서 새로운 리더가 될 브로커를 결정</span>
- 그러고 나서 새로운 상태를 주키퍼에 쓴 뒤, 새로 리더가 할당된 파티션의 레플리카를 포함하는 모든 브로커에 **LeaderAndISR 요청을 보냄**

---

> LeaderAndISR 요청은 각 파티션의 데이터의 리더쉽과 복제를 관리하기 위해 Kafka 브로커들 사이에서 사용되는 통신 메커니즘

---

> - Apache Kafka와 LeaderAndISR 요청의 맥락에서 설명되는 **"팔로워"는 현재 리더가 아닌 파티션의 복제본을 의미**한다. Kafka에서 각 파티션은 하나의 리더와 여러 개의 팔로워를 가진다. **리더는** <span style='color:#f7b731'>읽기와 쓰기를 처리하는 역할</span>을 맡고, **팔로워**는 <span style='color:#f7b731'>오류 허용 및 고가용성을 위해 리더의 데이터를 복제</span>한다. 

---

> 브로커가 특정 파티션에 대한 팔로워가 될 때는 파티션의 데이터 복사본을 유지하고 리더와 동기화 상태를 유지하는 것을 의미한다. 팔로워는 리더가 실패했을 때 데이터 내구성과 가용성을 보장하기 위해 중요하다. 리더를 사용할 수 없게 되면 **팔로워 중 한 명이 새로운 리더가 되도록 승격**시켜 <span style='color:#f7b731'>서비스의 지속성을 보장</span>할 수 있다.

---

#### 요약
- 컨트롤러는 브로커가 클러스터에 추가되거나 재거될때 <span style='color:#f7b731'>파티션과 레플리카 중에서 리더를 선출할 책임</span>을 짐 
- 컨트롤러는 <span style='color:#f7b731'>서로 다른 2개의 브로커가 자신이 현재 컨트롤러라 생각하는 스플릿 브레인 현상을 방지</span>하기 위해 **에포크 번호를 사용**
- https://www.owl-dev.me/blog/73

---

### Kraft
- https://www.confluent.io/ko-kr/blog/removing-zookeeper-dependency-in-kafka/
---

아파치 카프카 커뮤니티는 19년도 부터 <span style='color:#f7b731'>기존의 주키퍼 기반 컨트롤러의 한계점을 극복</span>하기위해 <span style='color:#f7b731'>래프트 기반 컨트롤러 쿼럼</span>의 **KRaft**를 개발하였으며 3.3버전 이후 정식으로 출시
- https://raft.github.io/
- 
> Raft 알고리즘이란?
> - Paxos(팩소스) 알고리즘의 대안으로 설계된 분산합의 알고리즘
> - 노드 간에 안전하게 로그를 복제하고, 리더를 선출하여 클라이언트 요청을 처리함으로써 합의를 달성하는 것이 주요 목표
> - [Algorithm Raft Algorithm 소개](https://t3guild.wordpress.com/2020/03/28/algorithm-raft-algorithm/)

---

#### Kraft 개발 이유 
- 컨트롤러가 주키퍼에 메타데이터를 쓰는 작업은 동기적으로 이루어지지만, <span style='color:#f7b731'>브로커에 메시지를 보내는 작업</span>과 <span style='color:#f7b731'>주키퍼로부터 업데이트를 받는 과정이 비동기</span>이기 때문에 **브로커, 컨트롤러, 주키퍼 간에 메타데이터 불일치 발생** 가능
- 컨트롤러가 재시작될 때마다 모든 브로커와 파티션에 대한 메타데이터를 읽어와야함 
> [컨플루언트의 블로그](https://www.confluent.io/ko-kr/blog/removing-zookeeper-dependency-in-kafka/)에서는  다음과 같이 말하고 있음
   Actually, the problem is not with ZooKeeper itself but with the concept of external metadata management.(사실 문제는 ZooKeeper 자체가 아니라 외부 메타데이터 관리 개념에 있습니다.)

---

- **Kraft** 설계의 핵심 아이디어는 **카프카 그 자체**에 <span style='color:#f7b731'>사용자가 상태를 이벤트 스트림으로 나타 낼 수 있도록 하는 로그 기반 아키텍처를 도입</span>하는 것

---
#### 장점
- 다수의 컨슈머를 사용하여 이벤트를 재생함으로써 **최신 상태를 빠르게 따라 잡을 수 있음**
- 로그는 이벤트 사이에 명확한 순서를 부여하고 **컨슈머들이 항상 하나의 타임라인을 따라 움직이도록 보장**해줌
- 컨트롤러 노드들은 메타데이터 이벤트 로그를 관리하는 래프트 쿼럼이 되는데, 이 로그는 메타 데이터의 변경 내역을 저장함
	- 토픽, 파티션 등 주키퍼에 저장되어 있는 모든 정보를 저장

---

- 래프트 알고리즘을 사용함으로써 컨트롤러 노드들은 외부 시스템에 의존하지 않고 **자체적으로 리더를 선출** 가능
- 메타데이터 로그의 리더 역할을 맡고 있는 컨트롤러를 **액티브 컨트롤러**라고 부름
	- 액티브 컨트롤러는 모든 RPC(remote procedure call) 호출을 처리함
- **팔로워 컨트롤러**들은 <span style='color:#f7b731'>액티브 컨트롤러에 쓰여진 데이터들을 복제</span>하며, <span style='color:#f7b731'>액티브 콘트롤러에 장애가 발생했을 시 즉시 투입될 수 있도록 준비상태를 유지</span>
 ➡️ <mark style='background:#8854d0'>컨트롤러들이 모두 최신 상태를 가지고 있기 때문에, 컨트롤러 장애 복구는 모든 상태를 새 컨트롤러로 이전하는 리로드 기간을 필요로 하지 않음</mark>

---

- 컨틀롤러가 브로커에 <span style='color:#f7b731'>변경 사항을 push 하는 것이 아닌</span> 브로커들이 액티브 컨틀롤러로부터 **변경 사항을 pull**함
- 컨트롤러 쿼럼으로의 마이그레이션 작업의 일부로서, 기존에는 <span style='color:#f7b731'>주키퍼와 직접 통신하던 모든 클라이언트, 브로커 작업들은 이제 컨트롤러로 보내지게 됨</span>
➡️ <mark style='background:#8854d0'>이렇게 함으로써<span style='color:#f7b731'> 브로커 쪽에는 아무것도 바꿀 필요 없이</span>, 컨트롤러만을 바꿔 주는 것만으로 매끄러운 마이그레이션이 가능해짐</mark>

---

#### 어떻게 Kraft로 옮겨갈 수 있을까?
- chapter 6 155 p.

---

### 복제

---

- **복제(Replica)** 는 <span style='color:#f7b731'>카프카 아키텍처의 핵심</span>이다.
- 복제는 <span style='color:#f7b731'>개별적인 서버 노드에 장애가 발생할 때</span> 카프카가 **신뢰성과 지속성을 보장**하는 방법이기에 매우 중요하다.

---

1. 카프카에 저장되는 데이터는 토픽을 단위로 구성
2. 각 토픽은 1개 이상의 파티션으로 분할
3. 각 파티션은 다수의 **레플리카**를 가질 수 있음
4. 레플리카는 브로커에 저장되며, 대게 하나의 브로커는 수백 개에서 많게는 수천 개까지의 **레플리카를 저장**

---

#### 레플리카의 종류
- **리더 레플리카**
	- **각 파티션은 하나의 리더 레플리카**를 가짐
	- 일관성 보장을 위해 모든 쓰기 요청은 리더 레플리카에게 주어짐
		- 읽어올 때는 리더 레플리카, 팔로워 레플리카 둘다 사용
	- 팔로어 중 어느 것이 최신의 리더 상태를 유지하고 있는지 확인함
		- 각 팔로워가 마지막으로 요청한 오프셋 값을 확인함으로써 각 팔로워의 뒤처짐을 알 수 있음

---

- **팔로워 레플리카**
	- 리더 레플리카를 제외한 **나머지 레플리카**
	- 주 목표는 팔로워들은 <span style='color:#f7b731'>리더 레플리카로부터 최근 메시지를 복제하여 최신 상태를 유지</span>하는 것
	- 특정 파티션에서 리더 레플리카가 중단되는 경우 **팔로워 레플리카 중 하나**가 <span style='color:#f7b731'>해당 파티션의 새로운 리더로 선출</span>

---

- 10초 이상의 시간 동안에 가장 최근의 메시지를 복제하지 못한다면 **동기화되지 않는 것(out-sync)** 으로 간주되며 해당 레플리카는 새로운 리더가 될 수 없음
- 이와 반대로 최신 메시지를 계속 요청하는 팔로워 레플리카를 **동기화 레플리카(In-sync replica, ISR)** 이라고 하며 리더가 중단되는 경우에 동기화 레플리카만 리더로 선출될 수 있음

---

### 요청 처리

---

- 카프카 브로커가 하는 일의 대부분은 클라이언트와 파티션 레플리카 및 컨트롤러부터 <span style='color:#f7b731'>파티션 리더에게 전송되는 요청을 처리하는 것</span>이다.
- 카프카는 이러한 요청과 응답을 TCP로 전송하는 이진 프로토콜을 가지고 있다.
- ..?

---

#### 모든 요청에는 다음 내용을 포함하는 표준 헤더가 존재한다.
- **요청 유형**: API 키
- **요청 버전**: 프로토콜 API의 버전. 서로 다른 버전의 클라이언트로부터 요청을 받아 각각의 버전에 맞는 응답을 해줌
- **Correlation ID**: 각 요청의 고유한 식별자. 응답이나 에러 로그에도 포함되기 때문에 트러블슈팅에 사용 가능
- **클라이언트 ID**: 요청을 보낸 애플리케이션을 식별하기 위해 사용

---

#### 브로커의 내부
1. **브로커는** 연결을 받는 각 포트별로 <span style='color:#f7b731'>acceptor 스레드를 실행</span>하고, **acceptor 스레드**는 <span style='color:#f7b731'>연결을 생성하고 들어온 요청을 processor 스레드에 전달</span>
2. **processor 스레드(네트워크 스레드)** 는 클라이언트 연결로부터 요청을 받고 <span style='color:#f7b731'>요청 큐에 넣고</span>, <span style='color:#f7b731'>응답 큐에서 응답을 가져와 클라이언트 측으로 전송하는 일을 수행</span>한다. 
3. 요청 큐에 요청이 위치하면 **I/O 스레드(요청 핸들러 스레드)** 가 요청을 가져와 처리한다.

---

#### 가장 일반적인 형태의 클라이언트 요청
- 쓰기 요청
	- 카프카 브로커로 메시지를 쓰고 있는 프로듀서가 보낸 요청
- 읽기 요청
	- 카프카 브로커로부터 메시지를 읽어오고 있는 컨슈머나 팔로워 레플리카가 보낸 요청
- 어드민 요청
	- 토픽 생성이나 삭제와 같이 메타데이터 작업을 수행중인 어드민 클라이언트가 보낸 요청

---

#### 쓰기요청 
- **acks 설정 변수**는 쓰기 작업이 성공한 것으로 간주되기 전 <span style='color:#f7b731'>메시지에 대한 응답을 보내야 하는 브로커의 수</span>를 가리킴
	- `acks=1` : 리더만이 메시지를 받았을 때
	- `acks=all` : 모든 인-싱크 레플리카들이 메시지를 받았을 때
	- `acks=0` : 메시지가 보내졌을 때

---

- 파티션의 리더 레플리카를 가지고 있는 브로커가 해당 파티션에 대한 쓰기 요청을 받게되면 <span style='color:#f7b731'>몇 가지 유효성 검증을 수행</span>
	- 데이터를 보내고 있는 사용자에게 토픽에 대한 쓰기 권한이 있는가?
	- acks 설정 값이 올바른가? (0, 1, or all)
	- acks 설정 값이 all일 경우 충분한 인-싱크 레플리카가 존재하는가?
		- 인-싱크 레플리카 수가 설정된 값 아래로 내려가면 새로운 새로운 메시지를 받지 않도록 브로커를 설정해 줄 수 있음

---

- 브로커는 새 메시지를 로컬 디스크에 씀 
	- 리눅스의 경우 메시지는 파일시스템 캐시에 쓰여지며, 진짜 디스크에 언제 반영될지는 보장 안됨
	- 카프카는 데이터가 디스크에 저장될 때까지 기다리지 않음
		- 즉, **메시지의 지속성을 위해 복제에 의존**함

---

#### 읽기 요청
- 브로커는 <span style='color:#f7b731'>쓰기 요청이 처리되는 것과 매우 유사한 방식</span>으로 읽기 요청을 처리함
1. 클라이언트는 브로커에 토픽, 파티션, 오프셋 목록에 해당하는 메시지들을 요청
	1. 클라이언트는 몇 개의 메시지를 전달 받을지 지정할 수 있음
2. **제로카피 기법**을 사용해 클라이언트에게 메시지를 전송 
	1. 파일 시스템에서 읽어온 메시지를 중간 버퍼를 거치지 않고 곧바로 네트워크 채널로 전송하기 때문에 데이터를 복사하고 메모리 상에 버퍼를 관리하기 위한 오버헤드가 사라짐
	2. 결과적으로 성능이 향상됨

---

- 클라이언트는 반환 데이터의 최대 크기도 지정할 수 있지만 최소 크기도 지정할 수 있음
	- 최소 크기를 지정해 줌으로써 빈번한 네트워크 통신을 줄일 수 있음
		- 예) 최소한 10KB가 되면 메시지를 전송
	- 또한 클라이언트가 마냥 기다리기만 해도 문제가 발생할 수 있으니, 브로커에게 요청을 할 때 타임 아웃 역시 지정 가능 
		- 예) x 밀리초가 지나도 지정한 만큼의 데이터가 쌓이지 않으면 그냥 전송

---

- 대부분의 클라이언트는 모든 인-싱크 레플리카에 쓰여진 메시지들만 읽을 수 있음
	- 리더 레플리카에만 존재하는 메시지를 읽을 수 있게 허용한다면 일관성을 잃을 수 있기 때문
	- 팔로워 레플리카에게 복제되지 않은 메시지를 어떤 컨슈머가 읽은 후 리더가 중단되면 해당 메시지를 가지고 있는 다른 파티션이 없기 때문에 그 메시지가 사라지게 되고 다른 컨슈머들은 유실된 메시지를 읽을 수 없음 
	- **replica.lag.time.max.ms**로 복제를 기다라는 최대 시간을 지정할 수 있다.
		- 이 시간 이상으로 지연되면 아웃-오브-싱크 레플리카가 됨
- 컨슈머가 매우 많은 수의 파티션으로부터 이벤트를 읽어오는 경우가 있을 때, 카프카는 읽기 세션 캐시를 사용함 

---

### 물리적 저장소

---

- 카프카의 기본 저장 단위는 **파티션 레플리카**이다.
- 파티션은 다른 브로커들 사이에 분리될 수 없으며, 같은 브로커의 서로 다른 디스크에 분할 저장되는 것도 불가능하다.
	- 따라서 하나의 파티션 크기는 단일 마운트 포인트에 사용 가능한 공간으로 제한된다.

---

#### 계층화된 저장소
- 카프카 3.0부터 파티션 데이터를 저장할 때 **계층화된 저장소 기능**을 사용. 그 이유는?
	- 파티션별로 저장 가능한 데이터에 한도가 있음. 최대 보존 기한과 파티션 수는 물리적인 디스크 크기에 제한을 받음.
	- 디스크와 클러스터 크기는 저장소 요구 조건에 의해 결정됨. 지연과 처리량이 주 고려사항일 경우 클러스터는 필요한 것 이상으로 커지는 경우가 많음. ➡️ 이는 비용과 직결
	- - 클러스터의 크기를 줄이거나 키울 때, 파티션의 위치를 다른 브로커로 옮기는데 걸리는 시간은 파티션의 수에 따라 결정됨. 클러스터가 작을 수록 더 유연해짐

---

- **계층화된 저장소 기능**에서 카프카 클러스터는 <span style='color:#f7b731'>저장소를 원격, 로컬로 나뉨</span>
	- **로컬 계층**은 기존의 카프카 저장소 계층과 똑같이 <span style='color:#f7b731'>브로커의 로컬 디스크를 사용</span> 
	- **새로운 원격 계층**은 <span style='color:#f7b731'>HDFS, S3와 같은 전용 저장소 시스템</span>을 이용
	- 로컬 저장소가 계층 저장소 보다 훨씬 비싸므로 사용자는 로컬 계층의 보존 기간은 몇 시간, 원격 계층의 보존 기간은 몇 일/몇 달로 길게 설정할 수 있음(계층별 서로다른 보존 정책 설정 가능)

---

- 계층화된 저장소의 도입으로 인한 성능 변환 및 여러 활용 사례에 대한 성능 측정
	- chapter 6 p. 171
➡️ 브로커들이 원격 저장소로 로그 세그먼트를 전송해야하기 때문에 지연성 관점에서는 단점이 있지만, 무한한 저장 공간, 더 낮은 비용, 탄력성 뿐만 아니라 오래된 데이터와 실시간 데이터를 읽는 작업을 분리시킬 수 있다는 점에서 더욱 유용하다.

---

#### 파티션 할당
- 사용자가 토픽을 생성하면, 카프카는 우선 이 파티션을 브로커 중 하나에 할당 
- 만약 브로커가 6개 있고 여기에 파티션이 10개, 복제 팩터가 3인 토픽을 생성하고자 한다면, 카프카는 30개의 파티션 레플리카를 브로커 6개에 할당해야함
- 파티션을 할당할 때 다음의 목표를 가짐

---

- 레플리카들을 가능한 한 브로커 간에 고르게 분산시킴
	- 위와 같은 경우에는 브로커 별로 5개의 레플리카를 할당해야함
- 각 파티션에 대해 각각의 레플리카가 서로 다른 브로커에 배치되도록 해야함
	- 만약 파티션 0의 리더가 브로커 2에 있다면, 팔로워들은 브로커 3과 4에 배치 가능
	- 하지만 2에 팔로워 브로커가 또 배치되거나 3에 두 개가 모두 배치될 수 없음
- 만약 브로커에 랙 정보가 설정되어 있다면, 가능한 한 각 파티션의 레플리카들을 서로 다른 랙에 할당해야함

---

- 이렇게 하기 위해 임의의 브로커(4라고 가정)부터 시작해서 <span style='color:#f7b731'>각 브로커에 라운드로빈 형식으로 파티션을 할당함으로써 리더</span>를 결정
	- 브로커가 6개이므로, 파티션 0의 리더는 브로커 4, 1은 브로커 5, 2는 브로커 0에 하는 식으로 할당
	- 파티션 0의 리더가 브로커 4에 배치되어져 있다면 첫 번째 팔로워는 브로커 5에, 두 번째는 브로커 0에 배치
	- 마찬가지로 파티션 1의 리더는 브로커5에, 첫 번째 팔로워는 브로커 0에, 두 번째는 브로커 1에 배치

---

1. 각 파티션과 레플리카에 올바른 브로커를 선택했다면, <span style='color:#f7b731'>파티션별로 독립적으로 수행</span>되는 새 파티션을 저장할 **디렉토리를 결정**해야함
2. 각 디렉토리에 저장되어져 있는 파티션의 수를 센 뒤, <span style='color:#f7b731'>가장 적은 파티션이 저장된 디렉토리에 새파티션을 저장</span>
	1. 만약 새로운 디스크를 추가할 경우 모든 새 파티션은 이 디스크에 생성됨

---

> - **디스크 공간에 주의**
> 파티션을 브로커에 할당해 줄 때 사용 가능한 공간이나 현재 부하 같은 것은 고려되지 않는 것과, 파티션을 디스크에 할당해 줄 때 디스크에 저장된 파티션의 수만이 고려될 뿐 크기는 고려되지 않는 것을 명심해야한다.

---

#### 파일 관리
- 카프카는 영구히 데이터를 저장하지도, 데이터를 지우기전에 모든 컨슈머들이 메시지를 읽어갈 수 있도록 기다리지도 않음
- 대신 카프카 운영자는 <span style='color:#f7b731'>각각의 토픽에 대한 보존 기한을 설정 가능</span>
	- 예시) "오래된 메시지 삭제" 혹은 "용량이 넘어가면 삭제"

---

- 하나의 파티션은 여러 개의 세그먼트로 분할되며, 기본적으로 각 세그먼트는 1GB 데이터 또는 최근 1주일치의 데이터 중 적은 쪽만큼을 저장
- 카프카가 파티션 단위로 메시지를 쓰는 만큼 각 세그먼트 한도가 다 차면 세그먼트를 닫고 새 새그먼트를 생성

---

- 현재 쓰여지고 있는 세그먼트를 **액티브 세그먼트**라 부름
- **액티브 세그먼트**는 <span style='color:#f7b731'>어떠한 경우에도 삭제되지 않음</span>
	- 로그 보존 기간을 하루로 설정했는데, 각 세그먼트가 5일치의 데이터를 저장하고 있는 경우, 세그먼트가 닫히기 전까지 데이터를 삭제할 수 없기 때문에 실제로는 5일치의 데이터가 보존됨
	- 만약 데이터를 1주일간 보존하도록 설정하고 매일 새 세그먼트를 생성한다면, 생성할 때 마다 가장 오래 된 세그먼트가 삭제됨

---

#### 파일 형식
- 세그먼트는 **하나의 데이터 파일 형태로 저장**되며, <span style='color:#f7b731'>파일 안에는 카프카의 메시지와 오프셋이 저장</span>됨
- 디스크에 저장되는 데이터의 형식은, <span style='color:#f7b731'>프로듀서가 브로커에게 전송하고 이후에 브로커가 컨슈머에게 전송하는 메시지의 형식과 동일</span>
	- <span style='color:#f7b731'>네트워크를 통해 전달되는 형식</span>과 <span style='color:#f7b731'>디스크에 저장되는 형식</span>을 **통일함**으로써 카프카는 컨슈머에게 메시지를 전송할 때 **제로카피 최적화** 달성 가능
	- <span style='color:#f7b731'>즉, 컨슈머에게 메시지를 전송할 때 별도의 버퍼 메모리를 사용하지 않고 디스크에서 바로 네트워크로 전송</span>하며, 프로듀서가 이미 압축해서 전송한 메시지의 **압축 해지와 재압축을 하지 않아도 됨**

---

- 카프카 메시지는 **사용자 페이로드**와 **시스템 헤더** 두 부분으로 나누어짐
- 사용자 페이로드는 키 값(선택 사항)과 벨류 값, 헤더 모음(선택 사항) 을 포함 함. 각각의 헤더는 키/벨류 순서쌍임

---

- 카프카 프로듀서는 <span style='color:#f7b731'>언제나 메시지를 배치 단위로 전송</span>함
	- 만약 하나의 메시지만을 보내고자 하면 배치는 약간의 오버헤드를 발생시킴
- **메시지를 배치 단위로 묶음**으로써 공간을 절약하게 되는 만큼 <span style='color:#f7b731'>네트워크 대역폭과 디스크 공간을 덜 사용</span>하게 됨
	- **lingert.ms=10**으로 설정해줌으로써 약간의 딜레이 추가로 더 많은 메시지들이 같은 배치로 묶일 확률을 증가시킴
- 메시지 배치 레더에 포함되는 것들: chapter6 p. 174, p. 175

---

#### 압착
- 카프카는 두 가지 보존 정책을 허용함
	- 삭제(delete) 보존 정책
		- 지정된 보존 기한 보다 더 오래 된 이벤트들을 삭제
	- 압착(compact) 보존 정책
		- 토픽에서 각 키의 최근 값만 저장
- 보존 기한과 압착 설정을 동시에 적용 가능(delete,compact)

---

#### 압착의 작동 원리
- 각 로그는 **클린(clean)** 과 **더티(dirty)** 의 두 영역으로 나누어 짐
- 클린
	- 이전에 압착된 적 없었던 메시지들이 저장됨. 이 영역은 하나의 키마다 하나의 값만을 포함함
		- 이 값은 이전 압착 작업 시점에서의 최신값이기도 함
- 더티
	- 마지막 압착 작업 이후 쓰여진 메시지들이 저장됨

---

- 압착 기능이 활성화 되어져 있다면, 각 브로커는 압착 매니저 스레드와 다수의 압착 스레드를 활용하여 압착 작업을 수행 
	- **압착 매니저 스레드(Compression Manager thread)** 는 압축 관련 작업을 관리하는 카프카 브로커 내의 전용 스레드(브로커 전체의 압축 작업을 감독하고 시작하는 중앙 조정자 역할을 함)
- 각 스레드는 <span style='color:#f7b731'>전체 파티션 크기 대비 더티 메시지의 비율이 높은 파티션을 골라 압착</span>한 후 클린 상태로 만듬

---

- 파티션을 압착하기 위해 클리너 스레드는 <span style='color:#f7b731'>파티션의 더티 영역을 읽어서 인-메모리 맵을 생성</span>
- 맵의 각 항목은 메시지 키의 16비트 해시와 같은 키 값을 가지는 이전 메시지의 8비트 오프셋으로 이루어짐
	- 즉, 맵의 각 항목은 24바이트 만을 사용

---

1. 클리너 스레드가 오프셋 맵을 생성한 다음, <span style='color:#f7b731'>클린 세그먼트들을 오래된 것부터 읽어들이면서 오프셋 맵의 내용과 대조</span>
2. 각각의 메시지에 대해, <span style='color:#f7b731'>해당 메시지의 키값이 현재 오프셋 맵에 저장되어 있는지 확인</span>함
	- 만약 **저장되어져 있지 않다면**, 방금 전 읽어드린 메시지의 벨류값은 <span style='color:#f7b731'>여전히 최신값이라는 의미</span>이기 때문에 <span style='color:#f7b731'>메시지는 교체용 세그먼트로 복사</span>됨
	- 만약 **저장되어져 있다면**, 파티션 내 같은 키값을 가졌지만 <span style='color:#f7b731'>새로운 벨류값을 갖는 메시지가 있다는 의미</span>이므로 <span style='color:#f7b731'>해당 메시지는 건너뜀</span>
3. 키값에 대한 최신 벨류값을 갖는 모든 메시지들이 복사되고 나면, 압착 스레드는 교체용 세그먼트와 원본 세그먼트를 바꾼 뒤 다음 세그먼트로 계속 진행
4. 작업이 완료되면, **키별로 하나의 메시지만이 남게됨**

---

#### 토픽은 언제 압착되는가?
- 압착 정책은 **액티브 세그먼트가 아닌** <span style='color:#f7b731'>세그먼트에 저장되어 있는 메시지만</span>이 압착의 대상임
- 기본적으로 카프카의 토픽 내용물이 <span style='color:#f7b731'>50% 이상이 더티 레코드인 경우</span>에만 압착을 시작
	- 지나치게 자주 압착하지 않으면서, 너무 많은 더티 레코드가 존재하지 않도록 함
	- 관리자들은 아래 두 설정으로 압착이 시작되는 시점을 조절 가능
- min.compaction.lag.ms
	- 메시지가 쓰여진 뒤 압착될 때까지 지나가야 하는 최소 시간
- max.compaction.lag.ms
	- 메시지가 쓰여진 뒤 압착이 가능해질 때까지 딜레이 될 수 있는 최대 시간
